# Data Ingestion Configuration
data_ingestion:
  stage: "raw"
  input:
    source: "local"  # or "s3"
    path: "data/raw" # or "s3://my-bucket/data/raw"
    file_pattern: "*.csv"
    max_files: 100
    chunksize: 10000  
  retry:
    attempts: 3
    delay_seconds: 5


# Data Cleaning Configuration
data_cleaning:
  stage: "clean"
  operations:
    drop_duplicates: true
    fill_columns: ["value"]


# Data Validation Configuration
data_validation:
  stage: "validate"
  checks:
    allow_nulls: false
    fail_fast: true
    column_ranges:
      value:
        min: 0
        max: 1000


# Data Transformation Configuration
data_transformation:
  stage: "transform"
  transformations: {}  # To be populated after EDA
  output:
    directory: "data/transformed"


# Feature Selection Configuration
feature_selection:
  stage: "select"
  strategy:
    method: "mutual_info"
    top_k: 10
    random_state: 42


# Feature Engineering Configuration
feature_engineering:
  stage: "engineer"
  operations:
    generate_polynomials: false
    interaction_terms: false
    lag_features:
      enabled: true
      lags: [1, 3, 5]
      target_column: "value"


# Model Training Configuration
model_training:
  stage: "train"
  model:
    type: ["arima", "lstm"]
    hyperparameters:
      n_estimators: 100
      max_depth: 5
      learning_rate: 0.01
  training:
    target_column: "label"
    test_size: 0.2
    random_state: 42


# Model Evaluation Configuration
model_evaluation:
  stage: "evaluate"
  evaluation:
    metric: "accuracy"
    cross_validation: true
    n_splits: 5


# Model Prediction Configuration
model_prediction:
  stage: "predict"
  paths:
    model: "data/models/latest_model.pkl"
    input_data: "data/transformed/test.csv"
    output_predictions: "data/predictions/predicted_output.csv"
  settings:
    batch_size: 512
